{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510b7d52",
   "metadata": {},
   "source": [
    "# Thermal Neural Networks as Neural Ordinary Differential Equation (in Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698f89e",
   "metadata": {},
   "source": [
    "This jupyter notebook showcases how to utilize a thermal neural network (TNN) on an exemplary data set when defined as a neural ordinary differential equation (NODE).\n",
    "During training of a NODE, the gradients for the gradient descent optimization are not obtained through truncated backpropagation through time, but rather through the adjoint sensitivity method.\n",
    "This modification requires an enhanced framework, which is provided by R. T. Q. Chen's [torchdiffeq](https://github.com/rtqichen/torchdiffeq)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d500214",
   "metadata": {},
   "source": [
    "The data set can be downloaded from [Kaggle](https://www.kaggle.com/wkirgsn/electric-motor-temperature).\n",
    "It should be placed in `data/input/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d96117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchdiffeq import odeint_adjoint, odeint\n",
    "from torchinfo import summary as ti_summary\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter as TorchParam\n",
    "import random\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef7289",
   "metadata": {},
   "source": [
    "## Data setup\n",
    "The data handling for NODEs here is a chunked and mini-batched way, which is more elaborate than it was shown for TNNs before.\n",
    "\n",
    "Therefore, a class-based setup is followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53203e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "\n",
    "    input_cols = []\n",
    "    target_cols = []\n",
    "    dataset_path = None\n",
    "    input_temperature_cols = []\n",
    "    pid = \"Not_Available\"\n",
    "    temperature_scale = 100  # in Â°C\n",
    "    black_list = []  # some columns need to be dropped after featurizing\n",
    "\n",
    "    def __init__(self, input_cols=None, target_cols=None):\n",
    "        # make it possible to load more input cols than supposed by class definition\n",
    "        input_cols = input_cols or self.input_cols\n",
    "        target_cols = target_cols or self.target_cols\n",
    "        self.data = pd.read_csv(self.dataset_path)\n",
    "        col_arrangement = input_cols + [self.pid] + target_cols\n",
    "        self.data = self.data.loc[:, [c for c in col_arrangement if c in self.data]]\n",
    "        # note, some features in input/target cols will only exist after featurizing!\n",
    "        self.input_cols = [c for c in input_cols if c in self.data]\n",
    "        self.target_cols = [c for c in target_cols if c in self.data]\n",
    "\n",
    "    @property\n",
    "    def temperature_cols(self):\n",
    "        return self.input_temperature_cols + self.target_cols\n",
    "\n",
    "    @property\n",
    "    def non_temperature_cols(self):\n",
    "        return [c for c in self.data\n",
    "                if c not in self.temperature_cols + [self.pid, 'train_'+self.pid]]\n",
    "\n",
    "    def get_pid_sizes(self, pid_lbl=None):\n",
    "        \"\"\"Returns pid size as pandas Series\"\"\"\n",
    "        pid_lbl = pid_lbl or self.pid\n",
    "        return self.data.groupby(pid_lbl)\\\n",
    "                        .agg('size').sort_values(ascending=False)\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Simple division by a scale, no offsets\"\"\"\n",
    "        # Be wary that changing target_cols in featurize()\n",
    "        #  and calling it after this normalize function will bring unexpected behavior\n",
    "        #  e.g., adding a temperature to target cols in featurize and calling it after normalize will have\n",
    "        #   that new target temperature normalized on its max value instead of temp_denom\n",
    "        nt_cols = [c for c in self.non_temperature_cols if c in self.data]\n",
    "        t_cols = [c for c in self.temperature_cols if c in self.data]\n",
    "        # some columns might only exist after featurize()\n",
    "        self.data.loc[:, t_cols] /= self.temperature_scale\n",
    "        self.data.loc[:, nt_cols] /= self.data.loc[:, nt_cols].abs().max(axis=0)\n",
    "\n",
    "    def get_profiles_for_cv(self, cv_lbl, kfold_split=4):\n",
    "        \"\"\"Given a cross-validation label and a table of profile sizes, return a tuple which associates\n",
    "        training, validation and test sets with profile IDs.\n",
    "\n",
    "        Args:\n",
    "            cv_lbl (str): Cross-validation label. Allowed labels can be seen in wkutils.config.\n",
    "            kfold_split (int, optional): The number of profiles per fold. Only active if cv_lbl=='kfold'. Defaults to 4.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: training, validation and test set lists of lists of profile IDs fanned out by fold.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class KaggleDataSet(DataSet):\n",
    "\n",
    "    input_cols = ['ambient', 'coolant', 'u_d',\n",
    "                  'u_q', 'motor_speed', 'i_d', 'i_q']\n",
    "    target_cols = ['pm', 'stator_yoke', 'stator_tooth', 'stator_winding']\n",
    "    input_temperature_cols = [\"ambient\", \"coolant\"]\n",
    "    # The following path might need to be replaced by the path on your system\n",
    "    dataset_path = Path().cwd() / \"data\" / \"input\" / \"measures_v2.csv\"\n",
    "    pid = \"profile_id\"\n",
    "    name = \"kaggle\"\n",
    "    sample_time = 0.5  # in seconds\n",
    "\n",
    "    def get_profiles_for_cv(self, cv_lbl, kfold_split=4):\n",
    "        pid_sizes = self.get_pid_sizes()\n",
    "        if cv_lbl == '1fold':\n",
    "            test_profiles = [[60, 62, 74]]\n",
    "            validation_profiles = [[4]]\n",
    "            train_profiles = [[p for p in pid_sizes.index.tolist()\n",
    "                              if p not in test_profiles + validation_profiles]]\n",
    "\n",
    "        else:\n",
    "            NotImplementedError(f\"cv '{cv_lbl}' is not implemented.\")\n",
    "\n",
    "        # print size of each fold\n",
    "        # for i, chunk in enumerate(test_profiles):\n",
    "        train_sample_size = pid_sizes.loc[test_profiles[0]].sum()\n",
    "        print(f'Fold {0} test size: {train_sample_size} samples '\n",
    "              f'({train_sample_size / pid_sizes.sum():.1%} of total)')\n",
    "\n",
    "        return train_profiles, validation_profiles, test_profiles\n",
    "\n",
    "    def featurize(self):\n",
    "        # extra feats (FE)\n",
    "        # it is highly advisable to call featurize and then normalize, not the other way around!\n",
    "        # Because featurize might mess with input and target cols\n",
    "        if {'i_d', 'i_q', 'u_d', 'u_q'}.issubset(set(self.data.columns.tolist())):\n",
    "            extra_feats = {'i_s': lambda x: np.sqrt((x['i_d']**2 + x['i_q']**2)),\n",
    "                           'u_s': lambda x: np.sqrt((x['u_d']**2 + x['u_q']**2))}\n",
    "        self.data = self.data.assign(**extra_feats).drop(columns=self.black_list)\n",
    "        self.input_cols = [\n",
    "            c for c in self.data if c not in self.target_cols + [self.pid]]\n",
    "        # rearrange\n",
    "        self.data = self.data.loc[:, self.input_cols +\n",
    "                                  [self.pid] + self.target_cols]\n",
    "\n",
    "\n",
    "class ChunkedKaggleDataSet(KaggleDataSet):\n",
    "    name = \"chunked_kaggle\"\n",
    "\n",
    "    def __init__(self, input_cols=None, target_cols=None, chunk_size=None):\n",
    "        \"\"\"Produce chunks/subsequences of each profile that act as new profiles with the same length\"\"\"\n",
    "        super().__init__(input_cols=input_cols, target_cols=target_cols)\n",
    "        p_len = 1  # in hours\n",
    "        chunk_size = chunk_size or int(p_len * 3600 / self.sample_time)\n",
    "        tra_l, val_l, tst_l = self.get_profiles_for_cv(cv_lbl='1fold')\n",
    "        tmp_profiles = [[df] if pid in val_l[0] + tst_l[0]\n",
    "                        else [df.iloc[n:min(n + chunk_size, len(df)), :]\n",
    "                              .assign(**{self.pid: pid + i * 1000})\n",
    "                              for i, n in enumerate(range(0, len(df), chunk_size), start=1)]\n",
    "                        for pid, df in self.data.groupby(self.pid)]\n",
    "        self.data = pd.concat([a for b in tmp_profiles for a in b],\n",
    "                              ignore_index=True)  # flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensor(profiles_list, _ds, device, pid_lbl=None):\n",
    "    \"\"\"From the tabular data where all measurement profiles are concatenated on top of each other, create a 3D tensor\"\"\"\n",
    "    pid_lbl = pid_lbl or _ds.pid\n",
    "    if len(profiles_list) == 0:\n",
    "        return None, None\n",
    "    # there are possibly multiple pid columns due to chunked training set\n",
    "    pid_lbls = [c for c in _ds.data if c.endswith(_ds.pid)]\n",
    "    # tensor shape: (#time steps, #profiles, #features)\n",
    "    tensor = np.full((_ds.get_pid_sizes(pid_lbl)[profiles_list].max(),\n",
    "                      len(profiles_list),\n",
    "                      _ds.data.shape[1] - len(pid_lbls)), np.nan)\n",
    "    for i, (pid, df) in enumerate(_ds.data.loc[_ds.data.loc[:, pid_lbl].isin(profiles_list), :]\n",
    "                                          .groupby(pid_lbl)):\n",
    "        tensor[:len(df), i, :] = df.drop(columns=pid_lbls).to_numpy()\n",
    "\n",
    "    sample_weights = 1 - np.isnan(tensor[:, :, 0])\n",
    "\n",
    "    tensor = np.nan_to_num(tensor).astype(np.float32)\n",
    "    tensor = torch.from_numpy(tensor).to(device)\n",
    "    sample_weights = torch.from_numpy(sample_weights).to(device)\n",
    "    return tensor, sample_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db87e5c",
   "metadata": {},
   "source": [
    "## Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some custom activation function\n",
    "class Biased_Elu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.elu(x) + 1\n",
    "\n",
    "\n",
    "# The TNN as NODE\n",
    "class AdjointConformTNN(nn.Module):\n",
    "\n",
    "    def __init__(self, u, n_targets, input_feats,\n",
    "                 temperature_cols, non_temperature_cols,\n",
    "                 n_virtual_temperatures=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = n_targets\n",
    "        virt_ext_output_size = n_targets + n_virtual_temperatures\n",
    "\n",
    "        # inverse thermal capacitances\n",
    "        self.caps = TorchParam(torch.Tensor(virt_ext_output_size))\n",
    "        nn.init.normal_(self.caps, mean=-7, std=0.5)\n",
    "\n",
    "        # therm. conductances\n",
    "        n_temps = len(temperature_cols) + n_virtual_temperatures\n",
    "        n_conds = int(0.5 * n_temps * (n_temps - 1))\n",
    "        self.conductance_net = nn.Sequential(\n",
    "            nn.Linear(len(input_feats) + virt_ext_output_size, 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2, n_conds),\n",
    "            Biased_Elu()\n",
    "        )\n",
    "\n",
    "        # populate adjacency matrix\n",
    "        self.adj_mat = np.zeros((n_temps, n_temps), dtype=int)\n",
    "        adj_idx_arr = np.ones_like(self.adj_mat)\n",
    "        triu_idx = np.triu_indices(n_temps, 1)\n",
    "        adj_idx_arr = adj_idx_arr[triu_idx].ravel()\n",
    "        self.adj_mat[triu_idx] = np.cumsum(adj_idx_arr) - 1\n",
    "        self.adj_mat += self.adj_mat.T\n",
    "        self.adj_mat = torch.from_numpy(self.adj_mat[:self.output_size, :])  # crop\n",
    "        self.n_temps = n_temps\n",
    "\n",
    "        # power losses\n",
    "        self.ploss = nn.Sequential(\n",
    "            nn.Linear(len(input_feats) + virt_ext_output_size, 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, virt_ext_output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.temp_idcs = [i for i, x in enumerate(\n",
    "            input_feats) if x in temperature_cols]\n",
    "        self.nontemp_idcs = [i for i, x in enumerate(\n",
    "            input_feats) if x in non_temperature_cols]\n",
    "\n",
    "        self.u = u\n",
    "\n",
    "    def forward(self, t: Tensor, x: Tensor) -> Tensor:\n",
    "\n",
    "        # plant input\n",
    "        inp = torch.FloatTensor(self.u[(t.detach().numpy()*2).astype(int), :, :])\n",
    "        # print(inp.shape)\n",
    "        temps = torch.cat([x, inp[:, self.temp_idcs]], dim=1)\n",
    "        all_input = torch.cat([inp, x], dim=1)\n",
    "        conducts = torch.abs(self.conductance_net(all_input))\n",
    "        power_loss = torch.abs(self.ploss(all_input))\n",
    "\n",
    "        temp_diffs = torch.sum(\n",
    "            (temps.unsqueeze(1) - x.unsqueeze(-1)) * conducts[:, self.adj_mat], dim=-1)\n",
    "        out = torch.exp(self.caps) * (temp_diffs + power_loss)\n",
    "\n",
    "        return torch.clip(out, -1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24329e",
   "metadata": {},
   "source": [
    "### Custom MSE Loss with Weights\n",
    "Even after chunking all profiles into same-size subsequences, there will still be some shorter profiles that will inherit a lot of NaNs.\n",
    "These are to be ignored by a weighted MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleWeightedMSELoss(torch.nn.MSELoss):\n",
    "\n",
    "    def forward(self, inputs, targets, sample_w):\n",
    "        y_true, y_pred = targets, inputs\n",
    "\n",
    "        squared_diffs = torch.nn.functional.mse_loss(\n",
    "            y_pred, y_true, reduction='none')\n",
    "        total_weight = sample_w[:, :, None]\n",
    "        weighted_mse = squared_diffs * total_weight\n",
    "        weighted_mse = weighted_mse.sum() / sample_w.sum()\n",
    "        return weighted_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038587d",
   "metadata": {},
   "source": [
    "##  Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11740783",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "N_BATCHES = 42\n",
    "CHUNK_SIZE = 1020\n",
    "\n",
    "# init data\n",
    "ds = ChunkedKaggleDataSet(chunk_size=CHUNK_SIZE)\n",
    "train_pid = ds.pid\n",
    "ds.featurize()\n",
    "ds.normalize()\n",
    "\n",
    "pid_sizes = ds.get_pid_sizes().to_dict()  # test and val set sizes\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Training parameters\n",
    "train_l, val_l, test_l = ds.get_profiles_for_cv(\"1fold\")\n",
    "opt_func = torch.optim.NAdam\n",
    "loss_func = SampleWeightedMSELoss\n",
    "n_epochs = 60\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "logs = {'loss_trends_train': [[] for _ in train_l],\n",
    "        'loss_trends_val': [[] for _ in val_l],\n",
    "        'models_state_dict': [],\n",
    "        }\n",
    "\n",
    "all_predictions_df = ds.data.loc[:, ds.target_cols + [ds.pid]]\n",
    "gtruth = all_predictions_df.copy()\n",
    "\n",
    "# iterate over k folds (here, only one fold, k = 1)\n",
    "for fold_i, (fold_train_profiles, fold_val_profiles, fold_test_profiles) \\\n",
    "        in enumerate(zip(train_l, val_l, test_l)):\n",
    "    fold_val_profiles = fold_val_profiles or []\n",
    "\n",
    "    # generate tensors\n",
    "    train_tensor, train_sample_weights = generate_tensor(fold_train_profiles, ds, device,\n",
    "                                                            pid_lbl=train_pid)\n",
    "    val_tensor, val_sample_weights = generate_tensor(fold_val_profiles or [], ds, device)\n",
    "    test_tensor, test_sample_weights = generate_tensor(fold_test_profiles, ds, device)\n",
    "\n",
    "    # generate time arrays\n",
    "    t_span_train = torch.arange(0.0, len(train_tensor), dtype=torch.float32).to(device) * ds.sample_time\n",
    "    t_span_test = torch.arange(0.0, len(test_tensor), dtype=torch.float32).to(device) * ds.sample_time\n",
    "    if val_tensor is not None:\n",
    "        t_span_val = torch.arange(0.0, len(val_tensor), dtype=torch.float32).to(device) * ds.sample_time\n",
    "\n",
    "    # model init\n",
    "    mdl = AdjointConformTNN(train_tensor[:, :, :len(ds.input_cols)],\n",
    "                            len(ds.target_cols), ds.input_cols,\n",
    "                            temperature_cols=ds.temperature_cols,\n",
    "                            non_temperature_cols=ds.non_temperature_cols).to(device)\n",
    "    \n",
    "    opt = opt_func(mdl.parameters(), lr=1e-2)\n",
    "    loss = loss_func().to(device)\n",
    "\n",
    "    pbar = trange(n_epochs, desc=f\"Seed {SEED}, fold {fold_i}\", position=fold_i, unit=\"epoch\")\n",
    "\n",
    "    if fold_i == 0:  # print only once\n",
    "        info_kwargs = {\"x\": train_tensor[0, :, -len(ds.target_cols):],\n",
    "                        \"t\": t_span_train[0]}\n",
    "        mdl_info = ti_summary(mdl, input_data=info_kwargs, device=device, verbose=0)\n",
    "        pbar.write(str(mdl_info))\n",
    "        logs['model_size'] = mdl_info.total_params\n",
    "    # it is important to transfer model to CPU right here, after model_stats were printed\n",
    "    #  otherwise, one model in a process might get back to GPU, whysoever\n",
    "    mdl.to(device)\n",
    "    \n",
    "    # generate shuffled indices in before hand\n",
    "    idx_mat = []\n",
    "    for i in range(n_epochs):\n",
    "        idx = np.arange(train_tensor.shape[1])\n",
    "        np.random.shuffle(idx)\n",
    "        idx_mat.append(idx)\n",
    "    idx_mat = np.vstack(idx_mat)\n",
    "    batch_size = np.ceil(train_tensor.shape[1] / N_BATCHES).astype(int)\n",
    "\n",
    "    # Training loop\n",
    "    start_time = pd.Timestamp.now().round(freq='S')\n",
    "    for epoch in pbar:\n",
    "        # shuffle profiles\n",
    "        indices = idx_mat[epoch]\n",
    "        train_tensor_shuffled = train_tensor[:, indices, :]\n",
    "        train_sample_weights_shuffled = train_sample_weights[:, indices]\n",
    "        for n in range(N_BATCHES):\n",
    "            # mini-batch training (do not consider all profiles at once)\n",
    "            train_tensor_shuffled_n_batched = \\\n",
    "                train_tensor_shuffled[:, n*batch_size:min((n+1)*batch_size, train_tensor_shuffled.shape[1]), :]\n",
    "            train_sample_weights_shuffled_n_batched = \\\n",
    "                train_sample_weights_shuffled[:,\n",
    "                                                n * batch_size:min((n+1)*batch_size,\n",
    "                                                                    train_sample_weights_shuffled.shape[1])]\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            mdl.u = train_tensor_shuffled_n_batched[:, :, :len(ds.input_cols)]\n",
    "            trajectory = odeint_adjoint(mdl, y0=train_tensor_shuffled_n_batched[0, :, -len(ds.target_cols):],\n",
    "                                        t=t_span_train, method='euler', adjoint_method=\"euler\",)\n",
    "\n",
    "            train_loss = loss(trajectory, train_tensor_shuffled_n_batched[:, :, -len(ds.target_cols):],\n",
    "                                train_sample_weights_shuffled_n_batched)\n",
    "            train_loss.backward()\n",
    "            opt.step()\n",
    "        with torch.no_grad():\n",
    "            logs[\"loss_trends_train\"][fold_i].append(train_loss.item())\n",
    "            pbar_str = f'Loss {train_loss.item():.2e}'\n",
    "        # validation set\n",
    "        if val_tensor is not None:\n",
    "            with torch.no_grad():\n",
    "                mdl.u = val_tensor[:, :, :len(ds.input_cols)]\n",
    "                val_traj = odeint(mdl, y0=val_tensor[0, :, -len(ds.target_cols):],\n",
    "                                    t=t_span_val, method=\"euler\")\n",
    "\n",
    "                # logging\n",
    "                val_loss = loss(val_traj, val_tensor[:, :, -len(ds.target_cols):],\n",
    "                                val_sample_weights).item()\n",
    "                logs[\"loss_trends_val\"][fold_i].append(val_loss)\n",
    "                pbar_str += f'| val loss {val_loss:.2e}'\n",
    "        pbar.set_postfix_str(pbar_str)\n",
    "\n",
    "    # test set evaluation\n",
    "    with torch.no_grad():\n",
    "        mdl.u = test_tensor[:, :, :len(ds.input_cols)]\n",
    "        test_traj = odeint(mdl, y0=test_tensor[0, :, -len(ds.target_cols):],\n",
    "                            t=t_span_test, method=\"euler\").detach().numpy()\n",
    "    # store test set predictions\n",
    "    for i, tst_set_id in enumerate(sorted(fold_test_profiles)):\n",
    "        row_mask = all_predictions_df.loc[:, ds.pid] == tst_set_id\n",
    "        all_predictions_df.loc[row_mask, ds.target_cols] = test_traj[:pid_sizes[tst_set_id], i, :]\n",
    "    # Save model to logs\n",
    "    logs[\"models_state_dict\"].append(mdl.state_dict())\n",
    "\n",
    "\n",
    "# filter prediction & ground truth placeholders for actual test set\n",
    "unrolled_test_profiles = [b for a in test_l for b in a]\n",
    "all_predictions_df = all_predictions_df.query(f\"{ds.pid} in @unrolled_test_profiles\")\n",
    "gtruth = gtruth.query(f\"{ds.pid} in @unrolled_test_profiles\")\n",
    "\n",
    "logs['all_predictions_df'] = all_predictions_df.assign(repetition=SEED)\n",
    "logs['ground_truth'] = gtruth\n",
    "logs['start_time'] = start_time\n",
    "logs['end_time'] = pd.Timestamp.now().round(freq='S')\n",
    "logs['seed'] = SEED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa8900",
   "metadata": {},
   "source": [
    "## Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85409562",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_profiles = unrolled_test_profiles\n",
    "\n",
    "fig, axes = plt.subplots(len(test_profiles), len(ds.target_cols), figsize=(20, 10))\n",
    "for i, (pid, y_test) in enumerate(gtruth.groupby(ds.pid)):\n",
    "    y_test *= 100\n",
    "    profile_pred = 100*all_predictions_df.loc[all_predictions_df[\"profile_id\"]==pid, ds.target_cols]\n",
    "    profile_pred = profile_pred.values[:len(y_test), :]\n",
    "    for j, col in enumerate(ds.target_cols):\n",
    "        ax = axes[i, j]\n",
    "        ax.plot(y_test.loc[:, col].reset_index(drop=True), color='tab:green')\n",
    "        ax.plot(profile_pred[:, j], color='tab:blue')\n",
    "        ax.text(x=0.5, y=0.8, \n",
    "                s=f'MSE: {((profile_pred[:, j] - y_test.loc[:, col])**2).sum() / len(profile_pred):.3f} KÂ²\\nmax.abs.:{(profile_pred[:, j]-y_test.loc[:, col]).abs().max():.1f} K',\n",
    "                transform=ax.transAxes)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f'Profile {pid}\\n Temp. in Â°C')\n",
    "        if i == len(test_profiles) - 1:\n",
    "            ax.set_xlabel(f'Iters')\n",
    "        elif i == 0:\n",
    "            ax.set_title(col)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7797872260fffc3150fb09aa844ee2632394704ff9dbe7473c5eb66442c979a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
